2017-3-28  
Virtual Assessment Network Meeting  
Location: https://bluejeans.com/124388012  

# Attendees 

| Name  | Organization  |  Role |   | 
|---|---|---|---|
| Karen Word  | University of California-Davis  | Postdoc, Lab for Data Intensive Biology  |   | 
| Kari L. Jordan  | Data Carpentry  | Deputy Director of Assessment  |  
| Beth Duckles  | Insightful, LLC  | Research Consultant  |   | 
| Alycia Crall  | National Ecological Observatory Network  | Science Educator/Evaluator  |   |
| Olav Vahtras  | KTH Royal Institute of Technology Sweden  | Professor of Theoretical Chemistry  |   |
| Amber Budden  | DataONE, UNM  | Dir Community Engagement/Outreach  |  |   
| Louisa Bellis | Elixir (University of Cambridge)  | Training Quality & Impact Coordinator  |   |

**Apologies**: Kim Gurwitz, H3ABioNet, University of Cape Town, South Africa

# Agenda
1. Introductions
  * **Amber**: Director of Community Engagement & Outreach with DataOne. Data discovery interface enabling search across repositories at the same time. Also does a lot of training in data management in best practices. Does evaluations of workshops on how materials are received as well as broader community-wide surveys.	
  * **Olav**: Chemistry professor in Sweden in computational science. Teaches in python, involved with software carpentry for a few years, has a new undergraduate course and would like to hear more about how people handle assessment of these types of 
courses
  * **Alycia**: NEON is collecting extensive data across the country at different locations. Because large datasets, provide training for people using those data. Also provide training on python, R, other programming languages. Currently leading evaluation effort for trainings. Also have online tutorials to teach these types of skills, and every summer have a week-long institute for early-career scientists to train them on current needs of the community. Fairly new with the project, have done a couple of evaluations, happy to share resources.
  * **Kari Jordan**: Deputy Director of Assessment for Data Carpentry. 
2. Review of Collaborations
  * **Kari**: has talked with some people in the network about using the Likert package in R. This is really good for using Likert scale items for assessment (agree-disagree). Has anyone else had experience using this package? It seems like there are more ways of using this.
  * **Alycia**: there were a few of us on the last call who aren’t as familiar with R. How do we get up to speed to be able to use these packages? 

  Kari: I got help from a DC community member. I started out just using R and using single plots .. e.g. the DC survey each question I would create a plot one by one. Boring! Ben Marwick? Volunteered from the Discuss list. Wrote out code, made 
comments for things that she would like to do, Ben commented, back and forth.   

Alycia has the transition been painful?  

Kari: Yes, for a few months primarily because I would start and stop for another project. ONce I dedicated a solid 2-3 days to learning this, I can reuse the same program and load in the new CSV to get my plots. [Repo.](https://github.com/carpentries/assessment)  

Alycia: NIMBioS developed a really great pool of resources for people who are just getting involved -- it even has an “evaluation 101 tutorial” They had a [conference](http://www.nimbios.org/IncludesConf/postconf) recently and may be offering additional evaluation workshops.  

Last call discussed collaboration with Sam Donovan who works with cubes -- she was the one that pulled this together. 

Curious about whether people have been developing logic models or theories of change or other instruments. Has been adding some of those resources to google drive -- have people been adding to those folders? 

Kari: Have not noticed a lot of resources, but please do be encouraged to add things to the drive! Right now there are some posters, some information about survey development and DC pre/post surveys in there. Right now we have a folder for meetings and their resources, a few for evaluation, some literature mainly about self-efficacy for open-source software, and then a poster. If there is information. If you add something then please add a note to the list (because no notifications). It would be great if we could all add pages documents and links that we find helpful to that drive.

Alycia: Will try to add things by the end of the week and send out a note to the group

3. Discussion
  *  **Skills-based MCQ’s**. When to use? Why to use? How to use?
DC went through a process of asking community to give multiple choice questions for python because we really want to know if there is a change in these skills before and after they attend the workshop. People have different opinions about multiple choice questions. Will this turn people away as a pre-assessment? Language to explain Thoughts?

Alycia: We developed pre and post knowledge questions for our workshop last year. We specifically stated that especially on the pre that we were just gathering information to find out everyone’s skill level to make sure we were meeting the needs of everyone. It also helped us to accept people (too many applicants) and have an idea of where people came from. Perspectives about “how comfortable were you” were on the post-survey only, not before, just in case through participation they became less comfortable?

Kari: So it wasn’t a paired survey then?

Alycia: The knowledge questions were paired, but with self-efficacy and skills, confidence, because their perception of their skills might have changed after participation, we asked that only in the post-survey. 

Amber: We do things like this at [DataOne](https://www.dataone.org/education-evaluation). We do pre- mid- and post-surveys with multichoice elements. Intern took a lot of research on survey design and question design and developed an evaluation tool that people can use to develop a set of questions for a survey instrument.

Kari: Language -- if you can easily answer these questions we would love it if you would’ve a workshop helper.

  * **Carpentry Long-Term Assessment Survey**
If you haven’t seen [this](https://www.surveymonkey.com/r/carpentrieslongtermassessment)
Will be collecting data until April 4. For anyone who took a workshop before December 2016. Are they still using the tools? Have they had any career opportunities because of the tools? The survey went out to about 4000 learners. We have about 415 responses now. Hoping to be able to show that workshops are working, that confidence has increased, they feel like they’ve belonged to the community. Please take survey if you’ve taken a workshop!

Alycia: what were you measuring?

Kari: We want to know that their data management practices have improved. Keeping raw data raw, for example. Maybe they didn’t know what “reproducibility” means, sharing data, sharing code, using programming languages, version control to share. Were they able to write their dissertation or conference paper or article because they were able to analyze data using tools learned in the workshops. Lots of writing questions, too. Look at using those data qualitatively. Have they followed up with other activities since the workshop? Did they join the carpentry community as a helper/instructor/committee member? Have they taken other workshops? Sought other online resources to further their learning?

Alycia: that sounds very similar to what we’re doing at NEON. It will be interesting to see if you’re asking questions in the same way as we are. It would be great if we could standardize so we can share data across comparisons.

Alycia: question -- has anyone spent any time thinking about success metrics? Is improving knowledge fine? Improving skills? Or are we looking at 90% show increases in knowledge? How specific are our targets? Our project is funded by NSF and they’re asking us to develop these success metrics. But not knowing what other people have achieved or how other people have designed these metrics… any thoughts on specific targets?

Kari: Had a difficult time because she wanted to have more metrics. When I approached the staff about those types of metrics, that’s not how they wanted to frame the conversation with the data. It’s a problem -- it would be good to have specific metrics that we can set to determine success, but we don't’ have those.

Amber: has metrics that are basically attendance numbers, number of events etc. We have outcomes that we are not required to report. (Also working with NSF). Some education activities that are a 2-hour session because there’s just no time. One thing we’re being asked to do is talk about impact, which is much harder to do. How has this training activity enabled someone to be more productive etc. That’s the level that we’re striving for, but impact comes at a much longer time scale than some of the metrics that we can more readily assess. NSF have been really talking about this recently. 

Alycia: NSF workshop really focused on collective impact, across multiple trainings and organizations. There’s a [link](https://collectiveimpactforum.org/) to a resource that was part of that conversation.

That might be a larger scale but this is a valuable resource.

Karen: Metrics are great and often arise out of qualitative methods.

Alycia: Books about mixed methods research discuss starting qualitative going quantitative and back and forth etc. Will try to look one up.

4. Adjournment

# Links
  * [NSF Includes Conference](http://www.nimbios.org/IncludesConf/postconf)
  * [Collective Impact Forum](https://collectiveimpactforum.org)
